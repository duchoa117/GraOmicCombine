# -*- coding: utf-8 -*-
"""cv_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/143h5-teEHmXFsGfE3Umbk5qLg_4Rs57u

## DeepSynergy

Author: Kristina Preuer

This Keras script shows how DeepSynergy was evaluated in one cross validation run (executed 5 times - looping over test folds). In this examples fold 0 is used for testing. The script uses 60% of the data  for training (folds 2, 3, 4) and 20% for validation (fold 1). The parameters are loaded with a separate text file (hyperparameters). Validation loss was used to determine the early stopping parameter. After hyperparameter selection the training and validation data was combined (80% = folds 1, 2, 3, 4) and the remaining 20% (fold 0) of the data were used for testing.

The original work was done accordingly with binet (https://github.com/bioinf-jku/binet/tree/master/binet).
"""

import os, sys
import pandas as pd
import numpy as np
import pickle
import gzip
import matplotlib.pyplot as plt
import tensorflow.compat.v1.keras as K
import tensorflow.compat.v1 as tf
from tensorflow.compat.v1.keras import backend
from tensorflow.compat.v1.keras.backend import set_session
from tensorflow.compat.v1.keras.models import Sequential
from tensorflow.compat.v1.keras.layers import Dense, Dropout
from tqdm.notebook import tqdm
from sklearn.metrics import r2_score, mean_squared_error


def pearson(y,f):
    rp = np.corrcoef(y, f)[0,1]
    return rp
def evaluate_model(x, y):
    p = model.predict(x)
    return y, p

"""#### Define parameters for this cross-validation run"""
data_names = ["all_test"]
for data_name in data_names:
    hyperparameter_file = 'hyperparameters' # textfile which contains the hyperparameters of the model
    data_file = f'{data_name}.pkl' # pickle file which contains the data (produced with normalize.ipynb)

    """#### Define smoothing functions for early stopping parameter"""

    def moving_average(a, n=3):
        ret = np.cumsum(a, dtype=float)
        ret[n:] = ret[n:] - ret[:-n]
        return ret[n - 1:] / n

    """#### Load parameters defining the model"""

    exec(open(hyperparameter_file).read())

    """#### Load data 
    tr = 60% of data for training during hyperparameter selection <br>
    val = 20% of data for validation during hyperparameter selection

    train = tr + val = 80% of data for training during final testing <br>
    test = remaining left out 20% of data for unbiased testing 

    splitting and normalization was done with normalize.ipynb
    """

    with open(data_file, "rb") as f:
        file = pickle.load(f)
        print(len(file))
    train_dc, y_train, test_dc, y_test, val_dc, y_val, mix_val, y_mix_val,\
    mix_test, y_mix_test, blind_cell_val, y_blind_cell_val,\
    blind_cell_test, y_blind_cell_test, blind_1_drug_val, y_blind_1_drug_val,\
    blind_1_drug_test, y_blind_1_drug_test, blind_1_drug_cell_val, y_blind_1_drug_cell_val,\
    blind_1_drug_cell_test, y_blind_1_drug_cell_test, blind_2_drug_val, y_blind_2_drug_val,\
    blind_2_drug_test, y_blind_2_drug_test, blind_all_val, y_blind_all_val,\
    blind_all_test, y_blind_all_test = file

    print(train_dc.shape, y_train.shape)
    print(val_dc.shape, y_val.shape)
    print(test_dc.shape, y_test.shape)

    """#### run set"""

    config = tf.ConfigProto(
            allow_soft_placement=True,
            gpu_options = tf.GPUOptions(allow_growth=True))
    set_session(tf.Session(config=config))

    train_dc

    model = Sequential()
    for i in range(len(layers)):
        if i==0:
            model.add(Dense(layers[i], input_shape=(train_dc.shape[1],), activation=act_func, 
                            kernel_initializer='he_normal'))
            model.add(Dropout(float(input_dropout)))
        elif i==len(layers)-1:
            model.add(Dense(layers[i], activation='linear', kernel_initializer="he_normal"))
        else:
            model.add(Dense(layers[i], activation=act_func, kernel_initializer="he_normal"))
            model.add(Dropout(float(dropout)))
        model.compile(loss='mean_squared_error', optimizer=K.optimizers.SGD(learning_rate=float(eta), momentum=0.5))

    """#### run model for hyperparameter selection"""

    hist = model.fit(train_dc, y_train, epochs=epochs, shuffle=True, batch_size=1024, validation_data=(val_dc, y_val))
    val_loss = hist.history['val_loss']
    model.reset_states()

    """#### smooth validation loss for early stopping parameter determination"""

    average_over = 1
    mov_av = moving_average(np.array(val_loss), average_over)
    smooth_val_loss = np.pad(mov_av, int(average_over/2), mode='edge')
    epo = np.argmin(smooth_val_loss)

    """#### determine model performance for methods comparison """

    hist = model.fit(train_dc, y_train, epochs=epo, shuffle=True, batch_size=1024, validation_data=(val_dc, y_val))
    test_loss = hist.history['val_loss']

    data_split_path = f"data_csv/{data_name}"
    train_dc_df = pd.read_csv(data_split_path+"_train_dc.csv")
    test_dc_df = pd.read_csv(data_split_path+"_test_dc.csv")
    val_dc_df = pd.read_csv(data_split_path+"_val_dc.csv")
    
    result_dict = {
        "test": (evaluate_model(test_dc, y_test), test_dc_df),
    }

    def r2_rmse( g ):
        r2 =  mean_squared_error( g['synergy'], g['predict'] )
        count = len(g['synergy'])
        rmse = np.sqrt( mean_squared_error( g['synergy'], g['predict'] ) )
        return pd.Series( dict(  r2 = r2, rmse = rmse, count = count ) )
    def get_top_data(r, df, top=10):
        G, P = r
        G = np.squeeze(G)
        P = np.squeeze(P)
        print(G.shape)
        print(P.shape)
        top = top*2
        abs_error = np.abs(G-P)
        index_top = abs_error.argsort()[:-1]
        values = abs_error[index_top]
        df_top = df.iloc[index_top]
        df_top["predict"] = P[index_top]
        df_top["abs_error"] = values
        return df_top

    for key, value in tqdm(result_dict.items()):
        temp = get_top_data(value[0], value[1])
        temp.to_csv(f"result_{key}_lastest.csv", index=False)
        print("_______________________________________________________")
