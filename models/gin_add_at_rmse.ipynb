{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hr2OuKpCq839"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from utils import *\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QmkPZtF6NoAr"
   },
   "outputs": [],
   "source": [
    "def return_omics_type(data):\n",
    "    t = 0\n",
    "    temp = []\n",
    "    name = \"_\"\n",
    "    for k, v in data.items():\n",
    "        t = t + v\n",
    "        if(v):\n",
    "            temp.append(k)\n",
    "    temp = tuple(temp)\n",
    "    if t == 1:\n",
    "        return \"1omics\", name.join(temp)\n",
    "    if t == 2:\n",
    "        return \"2omics\", name.join(temp)\n",
    "    if t == 3:\n",
    "        return \"3omics\", name.join(temp)\n",
    "        \n",
    "data_types = [\n",
    "    {\"ge\":1, \"mut\":1, \"meth\":1},\n",
    "    {\"ge\":1, \"mut\":1, \"meth\":0},\n",
    "    {\"ge\":1, \"mut\":0, \"meth\":1},\n",
    "    {\"ge\":0, \"mut\":1, \"meth\":1},\n",
    "    {\"ge\":1, \"mut\":0, \"meth\":0},\n",
    "    {\"ge\":0, \"mut\":1, \"meth\":0},\n",
    "    {\"ge\":0, \"mut\":0, \"meth\":1},    \n",
    "]\n",
    "data_sets = [\"all_test\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPBpKDG_NgwF"
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zIKk0zrkrF5A"
   },
   "outputs": [],
   "source": [
    "class GINConvNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1,num_features_xd=78, num_features_xt=25,\n",
    "                n_filters=32, embed_dim=128, output_dim=128, dropout=0.2,\n",
    "                out_tissue_d=13, ge=0, mut=0, meth=0):\n",
    "\n",
    "        super(GINConvNet, self).__init__()\n",
    "        self.ge = ge\n",
    "        self.mut = mut\n",
    "        self.meth = meth\n",
    "        print(self.ge, self.mut, self.meth)\n",
    "\n",
    "        dim = 32\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.n_output = n_output\n",
    "        # convolution layers\n",
    "        nn1 = Sequential(Linear(num_features_xd, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1_xd = Linear(dim, output_dim)\n",
    "\n",
    "        # cell line feature\n",
    "        # ge \n",
    "        if self.ge:\n",
    "            self.target_ge_cnv_block = Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=8, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters, out_channels=n_filters, kernel_size=8, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters, out_channels=n_filters*2, kernel_size=4),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*2, kernel_size=4),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*4, kernel_size=4),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(in_channels=n_filters*4, out_channels=n_filters*4, kernel_size=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Conv1d(in_channels=n_filters*4, out_channels=n_filters*2, kernel_size=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, output_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        # mut\n",
    "        if self.mut:\n",
    "            self.target_mut_cnv_block = Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters, out_channels=n_filters*2, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*4, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(2944, output_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        # meth\n",
    "        if self.meth:\n",
    "            self.target_meth_cnv_block = Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters, out_channels=n_filters*2, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Conv1d(in_channels=n_filters*2, out_channels=n_filters*4, kernel_size=8),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(3),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1280, output_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # synthetic omics data\n",
    "        total = self.ge + self.mut + self.meth\n",
    "        self.synthetic_omics = nn.Linear((total)*output_dim, 128)\n",
    "\n",
    "        #attension\n",
    "        self.key_xt = nn.Linear(output_dim, output_dim)\n",
    "        self.key_drug = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "        self.at_fc = nn.Linear(3*output_dim, 1)\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2*output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.out = nn.Linear(128, n_output)\n",
    "\n",
    "        # activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_1_batch = data.x_1_batch\n",
    "        x_1, edge_index_1,  = data.x_1, data.edge_index_1\n",
    "        x_2_batch = data.x_2_batch\n",
    "        x_2, edge_index_2,  = data.x_2, data.edge_index_2\n",
    "\n",
    "    # drug 1\n",
    "        x_1 = F.relu(self.conv1(x_1, edge_index_1))\n",
    "        x_1 = self.bn1(x_1)\n",
    "        x_1 = F.relu(self.conv2(x_1, edge_index_1))\n",
    "        x_1 = self.bn2(x_1)\n",
    "        x_1 = F.relu(self.conv3(x_1, edge_index_1))\n",
    "        x_1 = self.bn3(x_1)\n",
    "        x_1 = F.relu(self.conv4(x_1, edge_index_1))\n",
    "        x_1 = self.bn4(x_1)\n",
    "        x_1 = F.relu(self.conv5(x_1, edge_index_1))\n",
    "        x_1 = self.bn5(x_1)\n",
    "        x_1 = global_add_pool(x_1, x_1_batch)\n",
    "        x_1 = F.relu(self.fc1_xd(x_1))\n",
    "        x_1 = F.dropout(x_1, p=0.2, training=self.training)\n",
    "    # drug 2\n",
    "        x_2 = F.relu(self.conv1(x_2, edge_index_2))\n",
    "        x_2 = self.bn1(x_2)\n",
    "        x_2 = F.relu(self.conv2(x_2, edge_index_2))\n",
    "        x_2 = self.bn2(x_2)\n",
    "        x_2 = F.relu(self.conv3(x_2, edge_index_2))\n",
    "        x_2 = self.bn3(x_2)\n",
    "        x_2 = F.relu(self.conv4(x_2, edge_index_2))\n",
    "        x_2 = self.bn4(x_2)\n",
    "        x_2 = F.relu(self.conv5(x_2, edge_index_2))\n",
    "        x_2 = self.bn5(x_2)\n",
    "        x_2 = global_add_pool(x_2, x_2_batch)\n",
    "        x_2 = F.relu(self.fc1_xd(x_2))\n",
    "        x_2 = F.dropout(x_2, p=0.2, training=self.training)\n",
    "\n",
    "\n",
    "        # protein input feed-forward:\n",
    "        conv_xt_ge = torch.Tensor().to(device)\n",
    "        conv_xt_mut = torch.Tensor().to(device)\n",
    "        conv_xt_meth = torch.Tensor().to(device)\n",
    "\n",
    "        if self.mut:\n",
    "            target_mut = data.target_mut\n",
    "            target_mut = target_mut[:,None,:]\n",
    "            conv_xt_mut = self.target_mut_cnv_block(target_mut)\n",
    "\n",
    "        if self.meth:\n",
    "            target_meth = data.target_meth\n",
    "            target_meth = target_meth[:,None,:]\n",
    "            conv_xt_meth = self.target_meth_cnv_block(target_meth)\n",
    "\n",
    "        if self.ge:\n",
    "            target_ge = data.target_ge\n",
    "            target_ge = target_ge[:,None,:]\n",
    "            conv_xt_ge = self.target_ge_cnv_block(target_ge)\n",
    "        # 1d conv layers\n",
    "\n",
    "        xt = torch.cat((conv_xt_mut, conv_xt_meth, conv_xt_ge), 1)\n",
    "        xt = self.synthetic_omics(xt)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        key_xt = self.key_xt(xt)\n",
    "        key_drug_1 = self.key_drug(x_1)\n",
    "        key_drug_2 = self.key_drug(x_2)\n",
    "        \n",
    "        a_drug_1 = torch.exp(self.leaky_relu(self.at_fc(torch.cat((key_drug_1, key_xt, key_drug_2), 1))))\n",
    "        a_drug_2 = torch.exp(self.leaky_relu(self.at_fc(torch.cat((key_drug_2, key_xt, key_drug_1), 1))))\n",
    "        total = a_drug_1 + a_drug_2\n",
    "        a_drug_1 = torch.div(a_drug_1, total)\n",
    "        a_drug_2 = torch.div(a_drug_2, total)\n",
    "        x_1 = a_drug_1*x_1\n",
    "        x_2 = a_drug_2*x_2\n",
    "        x_drug_combine = x_1 + x_2\n",
    "\n",
    "        # concat\n",
    "        xc = torch.cat((x_drug_combine, xt), 1)\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "\n",
    "        return out, a_drug_1, a_drug_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jqz2LGdQuRQ_"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval, critation):\n",
    "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
    "    model.train()\n",
    "    avg_loss = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, x_1, x_2 = model(data)\n",
    "        loss = critation(output, data.y.view(-1, 1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "        text = 'Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                        batch_idx * len(data.x_1),\n",
    "                                                                        len(train_loader.dataset),\n",
    "                                                                        100. * batch_idx / len(train_loader),\n",
    "                                                                        loss.item())\n",
    "        print(text)\n",
    "    return sum(avg_loss)/len(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Iqc09aCuZKQ"
   },
   "outputs": [],
   "source": [
    "def predicting(model, device, loader, ats=False):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    if ats:\n",
    "        x_1_predicted = torch.Tensor()\n",
    "        x_2_predicted = torch.Tensor()\n",
    "        print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                output, x_1, x_2 = model(data)\n",
    "                total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "                total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "                x_1_predicted = torch.cat((x_1_predicted, x_1.cpu()), 0)\n",
    "                x_2_predicted = torch.cat((x_2_predicted, x_2.cpu()), 0)\n",
    "        return total_labels.numpy().flatten(),total_preds.numpy().flatten(), x_1_predicted, x_2_predicted\n",
    "    else:\n",
    "        print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                output, x_1, x_2 = model(data)\n",
    "                total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "                total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "        return total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Vn8jKcosUEA"
   },
   "outputs": [],
   "source": [
    "def draw(list_, label, y_label, title):\n",
    "    plt.figure()\n",
    "    plt.plot(list_, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    # save image\n",
    "    plt.savefig(title+\".png\")  # should before show method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ret(G, P):\n",
    "    return [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P)]\n",
    "\n",
    "def r2_rmse( g ):\n",
    "            r2 =  mean_squared_error( g['synergy'], g['predict'] )\n",
    "            count = len(g['synergy'])\n",
    "            rmse = np.sqrt( mean_squared_error( g['synergy'], g['predict'] ) )\n",
    "            return pd.Series( dict(  r2 = r2, rmse = rmse, count = count ) )\n",
    "        \n",
    "def get_top_data(r, df, top=10):\n",
    "    G, P, x_1_ats, x_2_ats = r\n",
    "    x_1_ats = np.array(x_1_ats)\n",
    "    x_2_ats = np.array(x_2_ats)\n",
    "    top = top*2\n",
    "    abs_error = np.abs(G-P)\n",
    "    index_top = abs_error.argsort()[:]\n",
    "    values = abs_error[index_top]\n",
    "    df_top = df.iloc[index_top].copy()\n",
    "    df_top[\"log_synergy\"] = G[index_top]\n",
    "    df_top[\"predict\"] = P[index_top]\n",
    "    df_top[\"abs_error\"] = values\n",
    "    df_top[\"x_1_ats\"] = x_1_ats[index_top]\n",
    "    df_top[\"x_2_ats\"] = x_2_ats[index_top]\n",
    "    return df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rth4mZ0BN4bn"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8qktZ2NoN3nN"
   },
   "outputs": [],
   "source": [
    "model_st = \"GINConvNet\"\n",
    "dataset = 'GDSC'\n",
    "train_batch = 1024\n",
    "val_batch = 1024\n",
    "test_batch = 1024\n",
    "log_interval = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMQ9mAaHN7Rs"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-fGib0aPW5H"
   },
   "source": [
    "## Define paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413417,
     "status": "ok",
     "timestamp": 1620975636029,
     "user": {
      "displayName": "Hoa D. Vu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi5xuU4jP5haCTK9aB8EdlS9KLrsyHO0J5s60I0xg=s64",
      "userId": "01639392045271123699"
     },
     "user_tz": -420
    },
    "id": "KvyarP1DJP4U",
    "outputId": "6f3012c7-6ca3-4a4c-e216-f81e20a5dc6f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epochs:  300\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "num_epoch = 300\n",
    "best_ret_test = None\n",
    "print('Learning rate: ', lr)\n",
    "print('Epochs: ', num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VniJdvO-PJ89"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in data_types:\n",
    "    for data_set in data_sets:\n",
    "        \n",
    "        data_path = f\"data/split_data/{data_set}\"\n",
    "        data_processed_path = \"data/split_data/{data_set}/processed/\"\n",
    "        model_st = \"GINConvNet\"\n",
    "        dataset = 'GDSC'\n",
    "        train_batch = 1024\n",
    "        val_batch = 1024\n",
    "        test_batch = 1024\n",
    "        log_interval = 20\n",
    "\n",
    "        print('\\nrunning on ', model_st + '_' + dataset )\n",
    "        train_data = TestbedDataset(root=data_path, dataset=dataset+\"_\"+'train_dc')\n",
    "        val_data = TestbedDataset(root=data_path, dataset=dataset+\"_\"+'val_dc')\n",
    "        test_data = TestbedDataset(root=data_path, dataset=dataset+\"_\"+'test_dc')\n",
    "        # make data PyTorch\n",
    "        # mini-batch processing ready\n",
    "        train_loader = DataLoader(train_data, batch_size=train_batch, shuffle=True, follow_batch=['x_1', 'x_2'])\n",
    "        val_loader = DataLoader(val_data, batch_size=val_batch, shuffle=False, follow_batch=['x_1', 'x_2'])\n",
    "        test_loader = DataLoader(test_data, batch_size=test_batch, shuffle=False, follow_batch=['x_1', 'x_2'])\n",
    "\n",
    "        print(device)\n",
    "        modeling = GINConvNet(**data_type)\n",
    "        model = modeling.to(device)\n",
    "\n",
    "        lr = 0.001\n",
    "        num_epoch = 300\n",
    "        best_ret_test = None\n",
    "        print('Learning rate: ', lr)\n",
    "        print('Epochs: ', num_epoch)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_pearsons = []\n",
    "\n",
    "        omics, name_omics = return_omics_type(data_type)\n",
    "        save_path = \"model/save_model/\" + f\"GIN_ADD_AT/{omics}/{name_omics}/{data_set}\" + \"/\"\n",
    "        print(save_path)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        best_mse = 1000\n",
    "        best_pearson = 1\n",
    "        best_epoch = -1\n",
    "\n",
    "        best_val_losses = 10000\n",
    "\n",
    "        ret_test_save = [1,1]\n",
    "\n",
    "        model_file_name = save_path + 'best_model' + '.model'\n",
    "        result_file_name = save_path + 'result_' + model_st + '_' + dataset +  '.csv'\n",
    "        loss_fig_name = save_path + 'model_' + model_st + '_' + dataset + '_loss'\n",
    "        pearson_fig_name = save_path + 'model_' + model_st + '_' + dataset + '_pearson'\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        for epoch in range(300):\n",
    "            print('Epoch {} | Training on {} samples...'.format(epoch, len(train_loader.dataset)))\n",
    "            avg_loss = []    \n",
    "            train_loss = train(model, device, train_loader, optimizer, epoch+1, log_interval, loss_fn)\n",
    "            #VAL:\n",
    "            G,P = predicting(model, device, val_loader)\n",
    "            ret = return_ret(G, P)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(ret[1])\n",
    "            val_pearsons.append(ret[2])\n",
    "\n",
    "            if ret[1]<best_val_losses:\n",
    "                best_val_losses = ret[1]\n",
    "                G_test,P_test = predicting(model, device, test_loader)\n",
    "                ret_test_save = return_ret(G_test, P_test)\n",
    "                print(\"RMSE all test improved\")\n",
    "                torch.save(model.state_dict(), model_file_name)\n",
    "\n",
    "            with open(result_file_name,'w') as f:\n",
    "                f.write('ret_test: '+','.join(map(str,ret_test_save)) +\"\\n\")\n",
    "\n",
    "            draw_loss(train_losses, val_losses, loss_fig_name)\n",
    "            draw_pearson(val_pearsons, pearson_fig_name)\n",
    "\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            log = [\n",
    "                train_losses, val_losses\n",
    "                ]\n",
    "\n",
    "            with open(save_path+ \"/log.pickle\", \"wb\") as f:\n",
    "                pickle.dump(log, f)\n",
    "\n",
    "        data_split_path = f\"data/split_data/{data_set}/\"\n",
    "        train_dc = pd.read_csv(data_split_path+\"train_dc.csv\")\n",
    "        test_dc = pd.read_csv(data_split_path+\"test_dc.csv\")\n",
    "        val_dc = pd.read_csv(data_split_path+\"val_dc.csv\")\n",
    "\n",
    "        result_dict = {\n",
    "            \"test\": (predicting(model, device, test_loader, ats=True), test_dc),\n",
    "        }\n",
    "\n",
    "        if os.path.exists(model_file_name):\n",
    "            model.load_state_dict(torch.load(model_file_name))\n",
    "\n",
    "        for key, value in tqdm(result_dict.items()):\n",
    "            temp = get_top_data(value[0], value[1])\n",
    "        temp.to_csv(save_path+\"detail_result.csv\", index=False)\n",
    "        \n",
    "        del model\n",
    "        del train_data \n",
    "        del val_data \n",
    "        del test_data \n",
    "        torch.cuda.empty_cache() \n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "gin.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "hoavd",
   "language": "python",
   "name": "hoavd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
